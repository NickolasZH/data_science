{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import zipfile\n",
    "import kaggle\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications.resnet import ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "# Define dataset paths\n",
    "DATASET_NAME = \"abhikjha/appa-real-face-cropped\"\n",
    "DATASET_PATH = \"appa_real_dataset\"\n",
    "ZIP_FILE = \"appa-real-face-cropped.zip\"\n",
    "\n",
    "# Check if the dataset is already downloaded\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    print(\"Downloading and extracting dataset...\")\n",
    "    os.system(f\"kaggle datasets download -d {DATASET_NAME} -p .\")\n",
    "    \n",
    "    if os.path.exists(ZIP_FILE):\n",
    "        with zipfile.ZipFile(ZIP_FILE, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(DATASET_PATH)\n",
    "        print(\"Dataset successfully extracted.\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Error: {ZIP_FILE} not found after download.\")\n",
    "else:\n",
    "    print(\"Dataset already exists. Skipping download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "image_dir = os.path.join(DATASET_PATH, \"final_files\", \"final_files\")  # Correct image directory\n",
    "labels_file = os.path.join(DATASET_PATH, \"labels.csv\")  # Correct labels file\n",
    "\n",
    "df = pd.read_csv(labels_file)  # Load age labels\n",
    "\n",
    "# Ensure that file paths are set only once\n",
    "df[\"file_name\"] = df[\"file_name\"].apply(lambda x: os.path.join(image_dir, os.path.basename(x)))\n",
    "\n",
    "df.to_csv(os.path.join(DATASET_PATH, \"labels.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load training and validation data\n",
    "def load_data(dataset_path, batch_size=32, img_size=(150, 150)):\n",
    "    df = pd.read_csv(os.path.join(dataset_path, \"labels.csv\"))\n",
    "    \n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1/255.,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "    \n",
    "    train_gen = datagen.flow_from_dataframe(\n",
    "        dataframe=df,\n",
    "        directory=None,\n",
    "        x_col=\"file_name\",\n",
    "        y_col=\"real_age\",\n",
    "        target_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='raw',\n",
    "        subset='training',\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    val_gen = datagen.flow_from_dataframe(\n",
    "        dataframe=df,\n",
    "        directory=None,\n",
    "        x_col=\"file_name\",\n",
    "        y_col=\"real_age\",\n",
    "        target_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='raw',\n",
    "        subset='validation',\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    return train_gen, val_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the age prediction model\n",
    "def create_model(input_shape):\n",
    "    backbone = ResNet50(input_shape=input_shape,\n",
    "                        weights='imagenet',\n",
    "                        include_top=False)\n",
    "    \n",
    "    # Unfreeze the last 15 layers for fine-tuning\n",
    "    for layer in backbone.layers[-15:]:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    model = Sequential([\n",
    "        backbone,\n",
    "        GlobalAveragePooling2D(),\n",
    "        BatchNormalization(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation='linear')  # Regression output\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001),\n",
    "        loss='mean_absolute_error',  # MAE loss for regression\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model\n",
    "def train_model(model, train_data, val_data, batch_size=32, epochs=30):\n",
    "    steps_per_epoch = len(train_data)\n",
    "    validation_steps = len(val_data)\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_data,\n",
    "        validation_data=val_data,\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_steps=validation_steps,\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6073 validated image filenames.\n",
      "Found 1518 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "# Load training and validation data\n",
    "train_data, val_data = load_data(DATASET_PATH, batch_size=32, img_size=(150, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nickolas/python_projects/practicum/.venv/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190/190 - 382s - 2s/step - loss: 15.4882 - mae: 15.4882 - val_loss: 16.6954 - val_mae: 16.6954\n",
      "Epoch 2/30\n",
      "190/190 - 351s - 2s/step - loss: 9.6292 - mae: 9.6292 - val_loss: 12.8929 - val_mae: 12.8929\n",
      "Epoch 3/30\n",
      "190/190 - 351s - 2s/step - loss: 8.7786 - mae: 8.7786 - val_loss: 11.9120 - val_mae: 11.9120\n",
      "Epoch 4/30\n",
      "190/190 - 353s - 2s/step - loss: 8.1172 - mae: 8.1172 - val_loss: 9.0173 - val_mae: 9.0173\n",
      "Epoch 5/30\n",
      "190/190 - 353s - 2s/step - loss: 7.6057 - mae: 7.6057 - val_loss: 11.0399 - val_mae: 11.0399\n",
      "Epoch 6/30\n",
      "190/190 - 353s - 2s/step - loss: 7.3612 - mae: 7.3612 - val_loss: 8.1152 - val_mae: 8.1152\n",
      "Epoch 7/30\n",
      "190/190 - 353s - 2s/step - loss: 7.0648 - mae: 7.0648 - val_loss: 7.2048 - val_mae: 7.2048\n",
      "Epoch 8/30\n",
      "190/190 - 354s - 2s/step - loss: 6.6182 - mae: 6.6182 - val_loss: 7.1888 - val_mae: 7.1888\n",
      "Epoch 9/30\n",
      "190/190 - 353s - 2s/step - loss: 6.4354 - mae: 6.4354 - val_loss: 7.4030 - val_mae: 7.4030\n",
      "Epoch 10/30\n",
      "190/190 - 353s - 2s/step - loss: 6.2419 - mae: 6.2419 - val_loss: 7.5963 - val_mae: 7.5963\n",
      "Epoch 11/30\n",
      "190/190 - 353s - 2s/step - loss: 6.0632 - mae: 6.0632 - val_loss: 6.8620 - val_mae: 6.8620\n",
      "Epoch 12/30\n",
      "190/190 - 353s - 2s/step - loss: 5.8380 - mae: 5.8380 - val_loss: 7.3103 - val_mae: 7.3103\n",
      "Epoch 13/30\n",
      "190/190 - 354s - 2s/step - loss: 5.6725 - mae: 5.6725 - val_loss: 7.1515 - val_mae: 7.1515\n",
      "Epoch 14/30\n",
      "190/190 - 353s - 2s/step - loss: 5.6000 - mae: 5.6000 - val_loss: 7.1365 - val_mae: 7.1365\n",
      "Epoch 15/30\n",
      "190/190 - 354s - 2s/step - loss: 5.4647 - mae: 5.4647 - val_loss: 7.3189 - val_mae: 7.3189\n",
      "Epoch 16/30\n",
      "190/190 - 353s - 2s/step - loss: 5.2614 - mae: 5.2614 - val_loss: 6.5182 - val_mae: 6.5182\n",
      "Epoch 17/30\n",
      "190/190 - 353s - 2s/step - loss: 5.0050 - mae: 5.0050 - val_loss: 7.2520 - val_mae: 7.2520\n",
      "Epoch 18/30\n",
      "190/190 - 353s - 2s/step - loss: 5.1349 - mae: 5.1349 - val_loss: 8.5135 - val_mae: 8.5135\n",
      "Epoch 19/30\n",
      "190/190 - 353s - 2s/step - loss: 4.8932 - mae: 4.8932 - val_loss: 6.5290 - val_mae: 6.5290\n",
      "Epoch 20/30\n",
      "190/190 - 353s - 2s/step - loss: 4.8437 - mae: 4.8437 - val_loss: 7.6329 - val_mae: 7.6329\n",
      "Epoch 21/30\n",
      "190/190 - 354s - 2s/step - loss: 4.7516 - mae: 4.7516 - val_loss: 6.5100 - val_mae: 6.5100\n",
      "Epoch 22/30\n",
      "190/190 - 353s - 2s/step - loss: 4.6865 - mae: 4.6865 - val_loss: 6.7571 - val_mae: 6.7571\n",
      "Epoch 23/30\n",
      "190/190 - 354s - 2s/step - loss: 4.5497 - mae: 4.5497 - val_loss: 7.1249 - val_mae: 7.1249\n",
      "Epoch 24/30\n",
      "190/190 - 354s - 2s/step - loss: 4.4611 - mae: 4.4611 - val_loss: 6.9456 - val_mae: 6.9456\n",
      "Epoch 25/30\n",
      "190/190 - 352s - 2s/step - loss: 4.5481 - mae: 4.5481 - val_loss: 6.3891 - val_mae: 6.3891\n",
      "Epoch 26/30\n",
      "190/190 - 353s - 2s/step - loss: 4.4504 - mae: 4.4504 - val_loss: 6.2916 - val_mae: 6.2916\n",
      "Epoch 27/30\n",
      "190/190 - 353s - 2s/step - loss: 4.3206 - mae: 4.3206 - val_loss: 6.4466 - val_mae: 6.4466\n",
      "Epoch 28/30\n",
      "190/190 - 354s - 2s/step - loss: 4.2563 - mae: 4.2563 - val_loss: 6.3574 - val_mae: 6.3574\n",
      "Epoch 29/30\n",
      "190/190 - 355s - 2s/step - loss: 4.0658 - mae: 4.0658 - val_loss: 6.3428 - val_mae: 6.3428\n",
      "Epoch 30/30\n",
      "190/190 - 354s - 2s/step - loss: 4.1274 - mae: 4.1274 - val_loss: 6.5244 - val_mae: 6.5244\n"
     ]
    }
   ],
   "source": [
    "# Create and train the model\n",
    "model = create_model((150, 150, 3))\n",
    "trained_model, history = train_model(model, train_data, val_data, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 - 13s - 270ms/step - loss: 6.5265 - mae: 6.5265\n",
      "Test MAE: 6.53\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_mae = trained_model.evaluate(val_data, verbose=2)\n",
    "print(f\"Test MAE: {test_mae:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
